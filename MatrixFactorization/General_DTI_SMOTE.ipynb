{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#######################################\n",
    "# SMOTE for DTI Project\n",
    "# Applied to: Enzyme Family\n",
    "# Generic for all 4 families (Enzyme, GPCR, Ion Channel, Nuclear Receptor)\n",
    "# Author: Vitor M. Silva\n",
    "# Date: 2025-04-30\n",
    "#######################################\n",
    "# Documentation\n",
    "#######################################\n",
    "'''\n",
    "# SMOTE (Synthetic Minority Over-sampling Technique) for DTI (Drug-Target Interaction)\n",
    "\n",
    "## Purpose\n",
    "SMOTE is an advanced over-sampling technique that creates **synthetic minority class examples** by interpolating between existing positive samples.\n",
    "\n",
    "In DTI prediction:\n",
    "- **Positives (class 1)**: Known drug-target interactions (rare).\n",
    "- **Negatives (class 0)**: Assumed non-interactions (abundant).\n",
    "\n",
    "Rather than duplicating existing positives, SMOTE generates **new positive vectors** by combining feature values from neighbors in latent space.\n",
    "\n",
    "## Algorithm Steps\n",
    "- Load the dataset (latent features + class).\n",
    "- Apply SMOTE to generate synthetic class 1 examples until class balance is achieved.\n",
    "- Concatenate synthetic data with the original dataset.\n",
    "- Save the augmented dataset for model training.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import keras\n",
    "from sklearn.utils import resample\n",
    "\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the ltype variable below to select the family "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for: nuclear_receptor\n"
     ]
    }
   ],
   "source": [
    "# Set base directory\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "ligands_type = ['enzyme', 'GPCR', 'ion_channel', 'nuclear_receptor']\n",
    "method_type = {\n",
    "    0: 'RandomOver',\n",
    "    1: 'RandomUnder',\n",
    "    2: 'TomekLinks',\n",
    "    3: 'CNNTomekLinks',\n",
    "    4: 'SMOTE',\n",
    "    5: 'SMOTETomekLinks',\n",
    "    6: 'BasicSMOTE',\n",
    "    7: 'ADASYN',\n",
    "    8: 'SMOTENC',\n",
    "    9: 'KMeansSMOTE',\n",
    "    10: 'LatentSampling',\n",
    "    11: 'NoiseInjection',\n",
    "    12: 'LaplacianWNN'\n",
    "}\n",
    "\n",
    "ltype_index = 3  # Change to 1, 2, or 3 for other families\n",
    "method_index = 4\n",
    "\n",
    "ltype = ligands_type[ltype_index]\n",
    "model_tag = method_type[method_index]\n",
    "\n",
    "# File paths\n",
    "original_file_name = 'final_new_par_50.csv'\n",
    "file_path = os.path.join(base_dir, 'data', 'split', ltype, original_file_name)\n",
    "\n",
    "# Load the data\n",
    "print(f\"Loading data for: {ltype}\")\n",
    "data_frame = pd.read_csv(file_path, header=None, skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original positives: 90\n",
      "Original negatives: 1314\n"
     ]
    }
   ],
   "source": [
    "# Split features and labels\n",
    "features = data_frame.iloc[:, :-1].values  # First 100 columns: 50 drug features + 50 target features\n",
    "labels = data_frame.iloc[:, -1].values     # Last column: label (0 or 1)\n",
    "\n",
    "# Separate positive and negative samples\n",
    "positive_features = features[labels == 1]\n",
    "negative_features = features[labels == 0]\n",
    "\n",
    "print(f\"Original positives: {len(positive_features)}\")\n",
    "print(f\"Original negatives: {len(negative_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features before SMOTE\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Apply SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(features_scaled, labels)\n",
    "\n",
    "# Inverse scaling to return features to original scale\n",
    "X_resampled_original_scale = scaler.inverse_transform(X_resampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine and save\n",
    "enhanced_data = np.hstack((X_resampled_original_scale, y_resampled.reshape(-1, 1)))\n",
    "enhanced_df = pd.DataFrame(enhanced_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save enhanced dataset\n",
    "enhanced_file_name = f'enhanced_{model_tag}_final_new_par_50.csv'\n",
    "enhanced_file_path = os.path.join(base_dir, 'data', 'split', ltype, enhanced_file_name)\n",
    "enhanced_df.to_csv(enhanced_file_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced dataset saved at: C:\\Users\\riskf\\OneDrive\\DTI - Data augmentation\\data\\split\\nuclear_receptor\\enhanced_SMOTE_final_new_par_50.csv\n",
      "New dataset size: (2628, 101)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Enhanced dataset saved at: {enhanced_file_path}\")\n",
    "print(f\"New dataset size: {enhanced_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original positives: 1314\n",
      "Original negatives: 1314\n"
     ]
    }
   ],
   "source": [
    "# Separate positive and negative samples\n",
    "positive_features_enhanced = enhanced_df[enhanced_df[100] == 1]\n",
    "negative_features_enhanced = enhanced_df[enhanced_df[100] == 0]\n",
    "\n",
    "print(f\"Original positives: {len(positive_features_enhanced)}\")\n",
    "print(f\"Original negatives: {len(negative_features_enhanced)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
