{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbaa6f17",
   "metadata": {},
   "source": [
    "# Computational prediction of drug-tager interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b35df5",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea5a73",
   "metadata": {},
   "source": [
    "Original article: Computational prediction of drug–target interactions using chemogenomic approaches: an empirical survey, A. Ezzat, others.\n",
    "<br>Data link: http://web.kuicr.kyoto-u.ac.jp/supp/yoshi/drugtarget/\n",
    "\n",
    "<p>\n",
    "\n",
    "Data supplement. Organic molecules (Qm9 file): https://deepchemdata.s3-us-west-.amazonaws.com/datasets/molnet_publish/qm9.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43855d",
   "metadata": {},
   "source": [
    "## 1 - Pre-setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f2f4b",
   "metadata": {},
   "source": [
    "### 1.1 - Imports (dependencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "from io import StringIO #retrive information for mlflow\n",
    "import sys #retrive information for mlflow\n",
    "\n",
    "\n",
    "#Chemistry Libraries\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "\n",
    "\n",
    "# Pubchem DB API https://pubchem.ncbi.nlm.nih.gov/compound/5388962\n",
    "import pubchempy as pcp # to retrive features and SMILES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067e936b",
   "metadata": {},
   "source": [
    "## 2 - Data imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cde6613",
   "metadata": {},
   "source": [
    "### 2.1 - Predicted drug-target interaction networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dec373",
   "metadata": {},
   "source": [
    "Cinq types de données.\n",
    "    <ul>Predicted compound-protein interacion pairs</ul>\n",
    "    <ul>Binary relation list of the gold standard drug-target interaction data</ul>\n",
    "    <ul>Adjacency matrix of the gold standard drug-target interaction data</ul>\n",
    "    <ul>Compound structure similarity matrix</ul>\n",
    "    <ul>Protein sequence similarity matrix</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beebc3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative paths. # Set directory paths for later use.\n",
    "# Get the directory of the script file\n",
    "base_dir = os.getcwd()\n",
    "base_dir\n",
    "\n",
    "ligants_type=['enzyme','GPCR','ion_channel','nuclear_receptor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fd0b8e",
   "metadata": {},
   "source": [
    "### 2.1.1 - GPCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eeebff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to read file at: C:\\Users\\riskf\\OneDrive\\DrugTargetSmilesBERT\\data\\split\\GPCR\\gpcr_admat_dgc.txt\n",
      "Trying to read file at: C:\\Users\\riskf\\OneDrive\\DrugTargetSmilesBERT\\data\\split\\GPCR\\gpcr_simmat_dc.txt\n",
      "Trying to read file at: C:\\Users\\riskf\\OneDrive\\DrugTargetSmilesBERT\\data\\split\\GPCR\\gpcr_simmat_dg.txt\n"
     ]
    }
   ],
   "source": [
    "#GPCR\n",
    "\n",
    "ltype=ligants_type[1]\n",
    "\n",
    "#set tables\n",
    "files_matrix_temp={'df_adjacency_matrix_GPCR_Y':'gpcr_admat_dgc.txt',\n",
    "       'df_similarity_matrix_GPCR_compound_St':'gpcr_simmat_dc.txt',\n",
    "       'df_similarity_matrix_GPCR_protein_Sd':'gpcr_simmat_dg.txt',\n",
    "       }\n",
    "\n",
    "df_temp_matrix={}\n",
    "\n",
    "\n",
    "for df_name, file_name in files_matrix_temp.items():\n",
    "    # Construct the file path using base_dir\n",
    "    file_path = os.path.join(base_dir,'data','split',ltype, file_name)\n",
    "\n",
    "    try:\n",
    "        # Read the file\n",
    "        print(\"Trying to read file at:\", file_path) # Print the path for verification\n",
    "        data_frame = pd.read_csv(file_path, delimiter='\\t', index_col=0)\n",
    "        df_temp_matrix[df_name] = data_frame\n",
    "    except FileNotFoundError:\n",
    "        print(f'File not found at the specified path: {file_path}')\n",
    "\n",
    "df_adjacency_matrix_GPCR_Y=df_temp_matrix['df_adjacency_matrix_GPCR_Y']\n",
    "df_similarity_matrix_GPCR_compound_St=df_temp_matrix['df_similarity_matrix_GPCR_compound_St']\n",
    "df_similarity_matrix_GPCR_protein_Sd=df_temp_matrix['df_similarity_matrix_GPCR_protein_Sd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950e20aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines (m): 95\n",
      "Columns (n): 223\n",
      "Size (m x n): 21185\n",
      "Known interactions: 635\n",
      "Known interactions (%): 2.9974%\n",
      "No interactions: 20550\n",
      "No interactions(%): 97.0026%\n"
     ]
    }
   ],
   "source": [
    "#Adjacent matrix. Y\n",
    "print('Lines (m): {}'.format(df_adjacency_matrix_GPCR_Y.shape[0]))\n",
    "print('Columns (n): {}'.format(df_adjacency_matrix_GPCR_Y.shape[1]))\n",
    "print('Size (m x n): {}'.format(df_adjacency_matrix_GPCR_Y.size))\n",
    "\n",
    "number_interactions_enzimes=(df_adjacency_matrix_GPCR_Y.values == 1).sum()\n",
    "print('Known interactions: {}'.format(number_interactions_enzimes))\n",
    "print('Known interactions (%): {:.4f}%'.format(number_interactions_enzimes/df_adjacency_matrix_GPCR_Y.size*100))\n",
    "print('No interactions: {}'.format(df_adjacency_matrix_GPCR_Y.size-number_interactions_enzimes))\n",
    "print('No interactions(%): {:.4f}%'.format((df_adjacency_matrix_GPCR_Y.size-number_interactions_enzimes)/df_adjacency_matrix_GPCR_Y.size*100))\n",
    "\n",
    "\n",
    "#print(df_adjacency_matrix_GPCR_Y.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "113bf83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines (m): 223\n",
      "Columns (n): 223\n",
      "Size (m x n): 49729\n"
     ]
    }
   ],
   "source": [
    "#Similarity Matrix Compound Columns\n",
    "print('Lines (m): {}'.format(df_similarity_matrix_GPCR_compound_St.shape[0]))\n",
    "print('Columns (n): {}'.format(df_similarity_matrix_GPCR_compound_St.shape[1]))\n",
    "print('Size (m x n): {}'.format(df_similarity_matrix_GPCR_compound_St.size))\n",
    "\n",
    "#print(df_simmilarity_matrix_GPCR_compound_Sd.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69b97470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines (m): 95\n",
      "Columns (n): 95\n",
      "Size (m x n): 9025\n"
     ]
    }
   ],
   "source": [
    "#Similarity Matrix Human Proteins Lines\n",
    "print('Lines (m): {}'.format(df_similarity_matrix_GPCR_protein_Sd.shape[0]))\n",
    "print('Columns (n): {}'.format(df_similarity_matrix_GPCR_protein_Sd.shape[1]))\n",
    "print('Size (m x n): {}'.format(df_similarity_matrix_GPCR_protein_Sd.size))\n",
    "\n",
    "#print(df_simmilarity_matrix_GPCR_protein_St.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe2b2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 32 iterations.: 968. Time elapsed: 32.70 seconds. Convergence -0.0010272026124482636\n",
      "Interaction 32/1000. Time elapsed: 33.61 seconds.\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix, diags\n",
    "from numpy.linalg import inv\n",
    "\n",
    "def calculate_objective(Y, A, B, W, Sd, St, lambda_l, lambda_d, lambda_t):\n",
    "    # Convert sparse matrices to dense arrays\n",
    "    Y_dense = Y.toarray() if hasattr(Y, \"toarray\") else Y\n",
    "    W_dense = W.toarray() if hasattr(W, \"toarray\") else W\n",
    "\n",
    "    #  error\n",
    "    diff_matrix = Y_dense - A.dot(B.T)\n",
    "    weighted_diff = W_dense * diff_matrix  # Element multiplication\n",
    "    recon_error = np.linalg.norm(weighted_diff, 'fro')**2\n",
    "\n",
    "    # Regularization terms\n",
    "    reg_A = lambda_l * np.linalg.norm(A, 'fro')**2\n",
    "    reg_B = lambda_l * np.linalg.norm(B, 'fro')**2\n",
    "\n",
    "    # Similarity error \n",
    "    AAT = A.dot(A.T)\n",
    "    Sd_dense = Sd.toarray() if hasattr(Sd, \"toarray\") else Sd\n",
    "    sim_error_d = lambda_d * np.linalg.norm(Sd_dense - AAT, 'fro')**2\n",
    "\n",
    "    BBT = B.dot(B.T)\n",
    "    St_dense = St.toarray() if hasattr(St, \"toarray\") else St\n",
    "    sim_error_t = lambda_t * np.linalg.norm(St_dense - BBT, 'fro')**2\n",
    "\n",
    "    return recon_error + reg_A + reg_B + sim_error_d + sim_error_t\n",
    "\n",
    "def update_A(Y, A, B, W, Sd, lambda_d, lambda_l, K):\n",
    "    m, n = Y.shape\n",
    "    A_new = np.zeros_like(A)\n",
    "    for i in range(m):\n",
    "        # Since Wi is a diagonal matrix formed from Wi, use Wi directly for weights\n",
    "        weights_i = W[i].toarray().squeeze()  # Convert to dense format and get as 1D array\n",
    "        \n",
    "        # For Yi and Sdi\n",
    "        Yi = Y[i].toarray().squeeze()\n",
    "        Sdi = Sd[i].toarray().squeeze()\n",
    "\n",
    "        # Initialize parts for summation\n",
    "        part1_sum = np.zeros((K,))\n",
    "        part2_sum = lambda_l * np.eye(K) + lambda_d * (A.T @ A)  # Regularization\n",
    "        \n",
    "        for j in range(n):\n",
    "            # Use the weight for the i drug and j target directly\n",
    "            weight_ij = weights_i[j]\n",
    "            part1_sum += weight_ij * Yi[j] * B[j, :]\n",
    "            part2_sum += weight_ij * np.outer(B[j, :], B[j, :])\n",
    "        \n",
    "        # Solve the linear system instead of direct inversion for numerical stability\n",
    "        A_new[i, :] = np.linalg.solve(part2_sum, part1_sum)\n",
    "    \n",
    "    return A_new\n",
    "\n",
    "\n",
    "def update_B(Y, A, B, W, St, lambda_t, lambda_l, K):\n",
    "    m, n = Y.shape  # Assuming Y has dimensions m x n\n",
    "    B_new = np.zeros_like(B)  # Initialize a new B matrix with the same shape as B\n",
    "    for j in range(n):\n",
    "        # Convert the j column of W to a dense format and extract as a 1D array\n",
    "        weights_j = W[:, j].toarray().squeeze()\n",
    "        \n",
    "        # Convert the j column of Y to a dense format as a 1D array\n",
    "        Yj = Y[:, j].toarray().squeeze()\n",
    "        \n",
    "        # Initialize the parts for summation\n",
    "        part1_sum = np.zeros((K,))\n",
    "        part2_sum = lambda_l * np.eye(K) + lambda_t * (B.T @ B)  # Regularizatio\n",
    "\n",
    "        for i in range(m):  # Iterate over drugs\n",
    "            # Use the extracted weight for the i drug and j target\n",
    "            weight_ij = weights_j[i]\n",
    "            part1_sum += weight_ij * Yj[i] * A[i, :]\n",
    "            part2_sum += weight_ij * np.outer(A[i, :], A[i, :])\n",
    "\n",
    "        # Solve the linear system instead of direct inversion for numerical stability\n",
    "        B_new[j, :] = np.linalg.solve(part2_sum, part1_sum)\n",
    "\n",
    "    return B_new\n",
    "\n",
    "def generate_final_dataset(A, B, Y):\n",
    "    final_dataset = []\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            features_drug = A[i, :]\n",
    "            features_target = B[j, :]\n",
    "            interaction_class = Y[i, j]\n",
    "            final_dataset.append(np.concatenate([features_drug, features_target, [interaction_class]]))\n",
    "    return np.array(final_dataset)\n",
    "\n",
    "m, n = 95, 223  # Y matrix dimentions\n",
    "K = 50  # feature dimension \n",
    "Y = csr_matrix(df_adjacency_matrix_GPCR_Y.values)  #  interaction matrix\n",
    "St = csr_matrix(df_similarity_matrix_GPCR_compound_St.values)  #drug similarity matrix - compound\n",
    "Sd = csr_matrix(df_similarity_matrix_GPCR_protein_Sd.values)  # target similarity matrix - protein\n",
    "W = csr_matrix(np.ones((m, n)))  # Example weight matrix\n",
    "A = np.random.rand(m, K)\n",
    "B = np.random.rand(n, K)\n",
    "count=0\n",
    "start_time = time.time()\n",
    "\n",
    "lambda_l, lambda_d, lambda_t = 0.1, 0.1, 0.01  # Regularization parameters\n",
    "tolerance = 1e-3  # Convergence \n",
    "max_iterations = 1000  # Maximum number of iterations\n",
    "\n",
    "objective_history = [calculate_objective(Y, A, B, W, Sd, St, lambda_l, lambda_d, lambda_t)]\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    A = update_A(Y, A, B, W, Sd, lambda_d, lambda_l, K)\n",
    "    B = update_B(Y, A, B, W, St, lambda_t, lambda_l, K)\n",
    "    \n",
    "    current_objective = calculate_objective(Y, A, B, W, Sd, St, lambda_l, lambda_d, lambda_t)\n",
    "    objective_history.append(current_objective)\n",
    "    \n",
    "    if np.abs(objective_history[-1] - objective_history[-2]) < tolerance:\n",
    "        print(f\"Converged after {iteration} iterations.\")\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Interaction {count}/{max_iterations}. Time elapsed: {elapsed_time:.2f} seconds.\", end='\\r')    \n",
    "        break\n",
    "    else:\n",
    "        #print(f\"Interaction {count}/1000. Max of 1000 interactions\")\n",
    "        converg=objective_history[-1] - objective_history[-2]\n",
    "        count +=1\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_interactions = max_iterations - count\n",
    "        print(f\"Interaction {count}/{max_iterations}. Remaining: {remaining_interactions}. Time elapsed: {elapsed_time:.2f} seconds. Convergence {converg}\", end='\\r')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "546b89cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(A)\n",
    "#print(B)\n",
    "#print(Y)\n",
    "\n",
    "df_a=pd.DataFrame(A)\n",
    "df_b=pd.DataFrame(B)\n",
    "df_y=pd.DataFrame(Y)\n",
    "\n",
    "file_path = os.path.join(base_dir,'data','split',ltype)\n",
    "file_path\n",
    "\n",
    "df_a.to_csv(file_path+'\\df_a.csv', index=False)\n",
    "df_b.to_csv(file_path+'\\df_b.csv', index=False)\n",
    "df_y.to_csv(file_path+'\\df_y.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddfe6787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset saved!\n"
     ]
    }
   ],
   "source": [
    "##OUTPUT Matrix. Important for next steps.\n",
    "final_dataset = generate_final_dataset(A, B, Y) \n",
    "final_df = pd.DataFrame(final_dataset)\n",
    "file_name=f'final_new_par_{K}.csv'\n",
    "file_path = os.path.join(base_dir,'data','split',ltype, file_name)\n",
    "output_path = file_path\n",
    "\n",
    "final_df.to_csv(output_path, index=False)\n",
    "print(\"Final dataset saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
