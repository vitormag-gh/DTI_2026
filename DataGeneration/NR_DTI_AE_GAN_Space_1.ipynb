{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riskf\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\riskf\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\Users\\riskf\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import keras\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models, backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN-GP Architecture\n",
    "def make_generator_model(input_dim, output_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, activation='relu', input_dim=input_dim))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(1024, activation='relu'))\n",
    "    model.add(layers.Dense(output_dim, activation='linear'))  # Linear activation for WGAN\n",
    "    return model\n",
    "\n",
    "def make_critic_model(input_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(512, activation='relu', input_dim=input_dim))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(1))  # No activation, linear output\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model(100, 101)\n",
    "critic = make_critic_model(101)\n",
    "\n",
    "# Losses and training\n",
    "def critic_loss(real_output, fake_output):\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "def gradient_penalty(batch_size, real_images, fake_images, critic):\n",
    "    epsilon = tf.random.normal([batch_size, 1], 0.0, 1.0)\n",
    "    interpolated = epsilon * real_images + (1 - epsilon) * fake_images\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        pred = critic(interpolated, training=True)\n",
    "    grads = tape.gradient(pred, [interpolated])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1]))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return gp\n",
    "\n",
    "def train_step(generator, critic, batch_size, generator_optimizer, critic_optimizer, real_features):\n",
    "    # Append a label column to real_features to match the critic's input expectations\n",
    "    labels = tf.ones((batch_size, 1))  # Assume label 1 for all positive samples\n",
    "    real_data = tf.concat([real_features, labels], axis=1)\n",
    "    \n",
    "    noise = tf.random.normal([batch_size, generator.input_shape[1]])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as crit_tape:\n",
    "        generated_data = generator(noise, training=True)\n",
    "\n",
    "        real_output = critic(real_data, training=True)\n",
    "        fake_output = critic(generated_data, training=True)\n",
    "\n",
    "        crit_loss = critic_loss(real_output, fake_output)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        penalty = gradient_penalty(batch_size, real_data, generated_data, critic)\n",
    "        crit_loss += 10 * penalty  # lambda for gradient penalty\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_critic = crit_tape.gradient(crit_loss, critic.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    critic_optimizer.apply_gradients(zip(gradients_of_critic, critic.trainable_variables))\n",
    "\n",
    "    return crit_loss, gen_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative paths. # Set directory paths for later use.\n",
    "# Get the directory of the script file\n",
    "base_dir = os.getcwd()\n",
    "ligants_type = ['enzyme', 'GPCR', 'ion_channel', 'nuclear_receptor']\n",
    "ltype = ligants_type[3]\n",
    "file_name = 'final_new_par_50.csv'\n",
    "file_path = os.path.join(base_dir, 'data', 'split', ltype, file_name)\n",
    "data_frame = pd.read_csv(file_path, header=None, skiprows=1)\n",
    "features = data_frame.iloc[:, :-1].values\n",
    "labels = data_frame.iloc[:, -1].values\n",
    "# Filter to get only the positive samples\n",
    "positive_features = features[labels == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 100)\n",
      "(1404, 100)\n",
      "(1404,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(positive_features.shape)\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "len(positive_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_samples_to_generate = 1224\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "learning_rate=0.0001\n",
    "beta_1=0.5\n",
    "generator_optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "critic_optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(0, len(positive_features), batch_size):\n",
    "        real_data_batch = positive_features[batch:batch + batch_size]\n",
    "        if real_data_batch.shape[0] != batch_size:  # Handle last batch which may be smaller\n",
    "            continue  # Skip if the batch isn't full size\n",
    "        crit_loss, gen_loss = train_step(generator, critic, batch_size, generator_optimizer, critic_optimizer, real_data_batch)\n",
    "        print(f'Epoch {epoch}, Batch {batch // batch_size}, Critic Loss: {crit_loss.numpy()}, Generator Loss: {gen_loss.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the last column name in  original dataframe represents the label\n",
    "all_column_names = data_frame.columns.tolist()  # This should have 101 names if the label is included in data_frame\n",
    "\n",
    "# Generate synthetic data\n",
    "noise = tf.random.normal([num_samples_to_generate, 100])\n",
    "synthetic_data = generator(noise, training=False)\n",
    "synthetic_data_df = pd.DataFrame(synthetic_data.numpy(), columns=all_column_names)\n",
    "# Set the label for all generated data to 1\n",
    "synthetic_data_df[all_column_names[-1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original and synthetic data\n",
    "enhanced_df = pd.concat([data_frame, synthetic_data_df], axis=0).reset_index(drop=True)\n",
    "file_name='enhanced_GAN_final_new_par_50_space_1.csv'\n",
    "file_path = os.path.join(base_dir,'data','split',ltype, file_name)\n",
    "output_path = file_path\n",
    "enhanced_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
