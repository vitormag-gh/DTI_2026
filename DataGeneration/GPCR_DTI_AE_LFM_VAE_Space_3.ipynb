{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riskf\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\riskf\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\Users\\riskf\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def build_encoder(latent_dim=20):\n",
    "    encoder_inputs = layers.Input(shape=(100,)) #change here features*2\n",
    "    x = layers.Dense(256, activation='relu')(encoder_inputs)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    z_mean = layers.Dense(latent_dim)(x)\n",
    "    z_log_var = layers.Dense(latent_dim)(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "    return encoder\n",
    "\n",
    "def build_decoder(latent_dim=20):\n",
    "    latent_inputs = layers.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(64, activation='relu')(latent_inputs)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    decoder_outputs = layers.Dense(100, activation='linear')(x) #change here features*2\n",
    "    decoder = models.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "    return decoder\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        # Initialize trackers for monitoring losses\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # Return list of metrics to be updated during training\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Compute KL divergence loss even during inference to track loss correctly\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1)\n",
    "        # Only add KL loss during training\n",
    "        if training:\n",
    "            self.add_loss(kl_loss)\n",
    "        return reconstructed\n",
    "\n",
    "    def train_step(self, data):\n",
    "    # Unpack the data\n",
    "        x = data[0] if isinstance(data, tuple) else data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(x, training=True)\n",
    "            reconstruction = self.decoder(z, training=True)\n",
    "\n",
    "            # If  data is flat (e.g., shape=(batch_size, features)), adjust axis accordingly\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                keras.losses.binary_crossentropy(x, reconstruction), axis=-1\n",
    "            )\n",
    "            reconstruction_loss = tf.reduce_sum(reconstruction_loss)  # Sum over all dimensions\n",
    "\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=-1\n",
    "            )\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        return {'loss': total_loss, 'reconstruction_loss': reconstruction_loss, 'kl_loss': kl_loss}\n",
    "\n",
    "# Example usage:\n",
    "latent_dim = 16  \n",
    "encoder = build_encoder(latent_dim)\n",
    "decoder = build_decoder(latent_dim)\n",
    "vae = VAE(encoder, decoder)\n",
    "#vae.compile(optimizer='adam')\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative paths. # Set directory paths for later use.\n",
    "# Get the directory of the script file\n",
    "base_dir = os.getcwd()\n",
    "base_dir\n",
    "ligants_type=['enzyme','GPCR','ion_channel','nuclear_receptor']\n",
    "ltype=ligants_type[1]\n",
    "file_name='final_new_par_LMF_50.csv'\n",
    "file_path = os.path.join(base_dir,'data','split',ltype, file_name)\n",
    "output_path = file_path\n",
    "data_frame = pd.read_csv(file_path, header=None, skiprows=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only include instances with label 1 (interactions)\n",
    "# Separate features and labels\n",
    "filtered_df = data_frame[data_frame.iloc[:, -1] == 1]  # All rows, all columns except the last one\n",
    "features_new = filtered_df.iloc[:, :-1]     # All rows, just the last column\n",
    "\n",
    "# Convert features DataFrame to a NumPy array if necessary\n",
    "x_train = features_new.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(635, 101)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para\n",
    "epochs=20\n",
    "batch_size=127"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "5/5 [==============================] - 1s 3ms/step - loss: -0.4868 - reconstruction_loss: -0.6322 - kl_loss: 0.0328\n",
      "Epoch 2/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.1129 - reconstruction_loss: -1.2570 - kl_loss: 0.0353\n",
      "Epoch 3/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.4838 - reconstruction_loss: -1.6597 - kl_loss: 0.0316\n",
      "Epoch 4/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.6422 - reconstruction_loss: -1.7564 - kl_loss: 0.0313\n",
      "Epoch 5/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.7031 - reconstruction_loss: -1.6868 - kl_loss: 0.0257\n",
      "Epoch 6/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.7515 - reconstruction_loss: -1.7531 - kl_loss: 0.0189\n",
      "Epoch 7/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.7848 - reconstruction_loss: -1.8019 - kl_loss: 0.0155\n",
      "Epoch 8/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8134 - reconstruction_loss: -1.8740 - kl_loss: 0.0146\n",
      "Epoch 9/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8157 - reconstruction_loss: -1.8370 - kl_loss: 0.0143\n",
      "Epoch 10/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8084 - reconstruction_loss: -1.7349 - kl_loss: 0.0140\n",
      "Epoch 11/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8396 - reconstruction_loss: -1.8813 - kl_loss: 0.0115\n",
      "Epoch 12/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8414 - reconstruction_loss: -1.8443 - kl_loss: 0.0103\n",
      "Epoch 13/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8510 - reconstruction_loss: -1.8927 - kl_loss: 0.0089\n",
      "Epoch 14/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8533 - reconstruction_loss: -1.8573 - kl_loss: 0.0081\n",
      "Epoch 15/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8531 - reconstruction_loss: -1.8712 - kl_loss: 0.0075\n",
      "Epoch 16/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8498 - reconstruction_loss: -1.8629 - kl_loss: 0.0071\n",
      "Epoch 17/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8559 - reconstruction_loss: -1.8592 - kl_loss: 0.0069\n",
      "Epoch 18/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8695 - reconstruction_loss: -1.9109 - kl_loss: 0.0062\n",
      "Epoch 19/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8756 - reconstruction_loss: -1.9560 - kl_loss: 0.0063\n",
      "Epoch 20/20\n",
      "5/5 [==============================] - 0s 2ms/step - loss: -1.8615 - reconstruction_loss: -1.9063 - kl_loss: 0.0064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ea9e19d8a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "vae.fit(x_train, epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only include instances with label 1 (interactions)\n",
    "filtered_df = data_frame[data_frame.iloc[:, -1] == 1]\n",
    "\n",
    "# Separate features\n",
    "features_new = filtered_df.iloc[:, :-1]  # Assuming the last column is the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = features_new.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 787us/step\n"
     ]
    }
   ],
   "source": [
    "z_mean, z_log_var, z = vae.encoder.predict(x_new)\n",
    "# Now, z contains the latent representations of  filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 790us/step\n"
     ]
    }
   ],
   "source": [
    "reconstructed_new = vae.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6   \\\n",
      "0   -0.622757 -0.427074 -0.546902 -0.681086 -0.616897 -0.399852 -0.646469   \n",
      "1   -0.432677 -0.327581 -0.411204 -0.477012 -0.441872 -0.262467 -0.407401   \n",
      "2   -0.432677 -0.327581 -0.411204 -0.477012 -0.441872 -0.262467 -0.407401   \n",
      "3   -0.432677 -0.327581 -0.411204 -0.477012 -0.441872 -0.262467 -0.407401   \n",
      "4   -0.420101 -0.217427 -0.451843 -0.451589 -0.261617 -0.356622 -0.460438   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "630 -0.549045 -0.404213 -0.456180 -0.632895 -0.563047 -0.400921 -0.567501   \n",
      "631 -0.553836 -0.410753 -0.452271 -0.635185 -0.561335 -0.401089 -0.567444   \n",
      "632 -0.528812 -0.379926 -0.418093 -0.607398 -0.547064 -0.384577 -0.551720   \n",
      "633 -0.531418 -0.384527 -0.433663 -0.608771 -0.546250 -0.395371 -0.549275   \n",
      "634 -0.622019 -0.426315 -0.545463 -0.684833 -0.615999 -0.395611 -0.650696   \n",
      "\n",
      "           7         8         9   ...        90        91        92  \\\n",
      "0   -0.473096 -0.398657 -0.460780  ...  0.339744  0.037264  0.269734   \n",
      "1   -0.382493 -0.215608 -0.354562  ...  0.353859  0.048259  0.203566   \n",
      "2   -0.382493 -0.215608 -0.354562  ...  0.411553  0.145124  0.286749   \n",
      "3   -0.382493 -0.215608 -0.354562  ...  0.423153  0.017025  0.178404   \n",
      "4   -0.451319 -0.280087 -0.304483  ...  0.438478  0.067663  0.203263   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "630 -0.449981 -0.465819 -0.427131  ...  0.333688  0.082358  0.252657   \n",
      "631 -0.451128 -0.468737 -0.426879  ...  0.333688  0.082358  0.252657   \n",
      "632 -0.414751 -0.449206 -0.421989  ...  0.370674  0.148754  0.291764   \n",
      "633 -0.435787 -0.459411 -0.409370  ...  0.380261  0.152017  0.278441   \n",
      "634 -0.475476 -0.404951 -0.462363  ...  0.339744  0.037264  0.269734   \n",
      "\n",
      "           93        94        95        96        97        98        99  \n",
      "0    0.471680  0.079110  0.437515  0.180993 -0.120987  0.001416 -0.090265  \n",
      "1    0.274057  0.186019  0.302913  0.136214  0.091484  0.120822  0.047419  \n",
      "2    0.284168  0.311242  0.341891  0.253709  0.247186  0.228189  0.217672  \n",
      "3    0.211208  0.187724  0.339084  0.074618  0.169961  0.180088  0.193297  \n",
      "4    0.186121  0.186558  0.326867  0.121006  0.291869  0.195145  0.340233  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "630  0.276907  0.231677  0.283591  0.188538  0.170798  0.156197  0.119306  \n",
      "631  0.276907  0.231677  0.283591  0.188538  0.170798  0.156197  0.119306  \n",
      "632  0.288028  0.304497  0.324409  0.249166  0.236031  0.222158  0.211160  \n",
      "633  0.275423  0.306790  0.322372  0.252641  0.234656  0.211461  0.214882  \n",
      "634  0.471680  0.079110  0.437515  0.180993 -0.120987  0.001416 -0.090265  \n",
      "\n",
      "[635 rows x 100 columns]\n",
      "           0         1         2         3         4         5         6   \\\n",
      "0   -0.767552 -1.027751 -0.528422 -1.121074 -0.360472 -0.633081 -0.486807   \n",
      "1   -0.523895 -0.431203 -0.421358 -1.115925 -0.642327 -0.796661 -0.634993   \n",
      "2   -0.775185 -1.118137 -0.672486 -0.925044 -0.772080 -0.521150 -0.291339   \n",
      "3   -0.554072 -0.115190 -0.502083 -0.271370 -0.569772 -0.519935 -0.550338   \n",
      "4   -0.509177 -0.288525 -0.163922 -0.259877 -0.334433 -0.407594 -0.160249   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "630 -0.407497 -0.271518 -0.193979 -0.262078 -0.429912 -0.353294 -0.159073   \n",
      "631 -0.588174 -0.400926 -0.566113 -0.692640 -0.233360 -1.004826 -0.385957   \n",
      "632 -0.478162 -0.528402 -0.454288 -0.594077 -0.402981 -0.513755 -0.444700   \n",
      "633 -0.674073 -0.545782 -0.313490 -0.402827 -0.731921 -0.774204 -0.333822   \n",
      "634 -0.494449 -0.776089 -0.239607 -0.549598 -0.563570 -0.314265 -0.201465   \n",
      "\n",
      "           7         8         9   ...        90        91        92  \\\n",
      "0   -0.843536 -0.820084 -0.759582  ...  0.547762  0.291385  0.293064   \n",
      "1   -0.422538 -0.314773 -0.629746  ...  0.377819  0.473200  0.495974   \n",
      "2   -0.893955 -1.031708 -0.669688  ...  0.582662  0.484859  0.204295   \n",
      "3   -0.619349 -0.445922 -0.328220  ...  0.168884  0.519407  0.311631   \n",
      "4   -0.461715 -0.379078 -0.196680  ...  0.153472  0.184376  0.249424   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "630 -0.315977 -0.197678 -0.162349  ...  0.134678  0.086805  0.119130   \n",
      "631 -0.513251 -0.799440 -0.470108  ...  0.061180  0.312376  0.240571   \n",
      "632 -0.478383 -0.817916 -0.287639  ...  0.144410  0.291012  0.109305   \n",
      "633 -0.570806 -0.433589 -0.361232  ...  0.278900  0.130306  0.246948   \n",
      "634 -0.570032 -0.660554 -0.433828  ...  0.248340  0.251928  0.037254   \n",
      "\n",
      "           93        94        95        96        97        98        99  \n",
      "0    0.271236  0.484003  0.188764  0.520099  0.485205  0.396466  0.517669  \n",
      "1    0.267658  0.276563  0.568462  0.483465  0.463516  0.437540  0.125336  \n",
      "2    0.187359  0.682945  0.130432  0.453610  0.227058  0.506430  0.731415  \n",
      "3    0.595487  0.394250  0.478263  0.513508  0.305145  0.350225  0.390605  \n",
      "4    0.348595  0.217766  0.080622  0.196463  0.151912  0.260311  0.229211  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "630  0.259772  0.198775  0.205416  0.132185  0.116273  0.222860  0.175101  \n",
      "631  0.360063  0.196086  0.363508  0.380330  0.442653  0.643408  0.373683  \n",
      "632  0.366629  0.212256  0.306501  0.244035  0.161014  0.386370  0.484587  \n",
      "633  0.362417  0.375730  0.212070  0.379376  0.251058  0.261913  0.182279  \n",
      "634  0.116072  0.320081  0.136221  0.215535  0.106520  0.234026  0.344538  \n",
      "\n",
      "[635 rows x 100 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert the reconstructed data to a DataFrame\n",
    "reconstructed_df = pd.DataFrame(reconstructed_new)\n",
    "\n",
    "# Display the first few rows of the reconstructed DataFrame\n",
    "print(pd.DataFrame(x_new))\n",
    "print(reconstructed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0         1         2         3         4         5         6    \\\n",
      "0 -0.767552 -1.027751 -0.528422 -1.121074 -0.360472 -0.633081 -0.486807   \n",
      "1 -0.523895 -0.431203 -0.421358 -1.115925 -0.642327 -0.796661 -0.634993   \n",
      "2 -0.775185 -1.118137 -0.672486 -0.925044 -0.772080 -0.521150 -0.291339   \n",
      "3 -0.554072 -0.115190 -0.502083 -0.271370 -0.569772 -0.519935 -0.550338   \n",
      "4 -0.509177 -0.288525 -0.163922 -0.259877 -0.334433 -0.407594 -0.160249   \n",
      "\n",
      "        7         8         9    ...       91        92        93        94   \\\n",
      "0 -0.843536 -0.820084 -0.759582  ...  0.291385  0.293064  0.271236  0.484003   \n",
      "1 -0.422538 -0.314773 -0.629746  ...  0.473200  0.495974  0.267658  0.276563   \n",
      "2 -0.893955 -1.031708 -0.669688  ...  0.484859  0.204295  0.187359  0.682945   \n",
      "3 -0.619349 -0.445922 -0.328220  ...  0.519407  0.311631  0.595487  0.394250   \n",
      "4 -0.461715 -0.379078 -0.196680  ...  0.184376  0.249424  0.348595  0.217766   \n",
      "\n",
      "        95        96        97        98        99   100  \n",
      "0  0.188764  0.520099  0.485205  0.396466  0.517669    1  \n",
      "1  0.568462  0.483465  0.463516  0.437540  0.125336    1  \n",
      "2  0.130432  0.453610  0.227058  0.506430  0.731415    1  \n",
      "3  0.478263  0.513508  0.305145  0.350225  0.390605    1  \n",
      "4  0.080622  0.196463  0.151912  0.260311  0.229211    1  \n",
      "\n",
      "[5 rows x 101 columns]\n"
     ]
    }
   ],
   "source": [
    "# Add a new column 'Label' with all values set to 1\n",
    "reconstructed_df[100] = 1\n",
    "\n",
    "# Display the first few rows to verify the new column\n",
    "print(reconstructed_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623/623 [==============================] - 0s 559us/step\n"
     ]
    }
   ],
   "source": [
    "num_samples_to_generate = 19915  # The number of new rows want to generate\n",
    "\n",
    "# Generate random samples from the latent space\n",
    "latent_dim = 16  # Ensure this matches the latent dimension size of  VAE\n",
    "z_new_samples = np.random.normal(size=(num_samples_to_generate, latent_dim))\n",
    "\n",
    "# Use the decoder to generate new data\n",
    "new_data_generated = vae.decoder.predict(z_new_samples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4639421 , -0.41055927, -0.1820075 , ...,  0.25906622,\n",
       "         0.29884842,  0.28503722],\n",
       "       [-0.54088026, -0.2505464 , -0.43269134, ...,  0.2952692 ,\n",
       "         0.5592264 ,  0.36449322],\n",
       "       [-0.3724004 , -0.3867712 , -0.34770685, ...,  0.23460422,\n",
       "         0.43924147,  0.21839231],\n",
       "       ...,\n",
       "       [-0.45168483, -0.5824973 , -0.28928208, ...,  0.25632712,\n",
       "         0.29967922,  0.28209397],\n",
       "       [-0.377553  , -0.4255461 , -0.15130213, ...,  0.28396446,\n",
       "         0.27018338,  0.3044331 ],\n",
       "       [-0.7326226 , -0.8165902 , -0.1666347 , ...,  0.12669772,\n",
       "         0.32729053,  0.3468352 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data_generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the generated data to a DataFrame\n",
    "new_data_df = pd.DataFrame(new_data_generated)\n",
    "\n",
    "# Add a column 'Label' with all values set to 1\n",
    "new_data_df[100] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_df = pd.concat([data_frame, new_data_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.622757</td>\n",
       "      <td>-0.427074</td>\n",
       "      <td>-0.546902</td>\n",
       "      <td>-0.681086</td>\n",
       "      <td>-0.616897</td>\n",
       "      <td>-0.399852</td>\n",
       "      <td>-0.646469</td>\n",
       "      <td>-0.473096</td>\n",
       "      <td>-0.398657</td>\n",
       "      <td>-0.460780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153380</td>\n",
       "      <td>0.292761</td>\n",
       "      <td>0.272396</td>\n",
       "      <td>0.308351</td>\n",
       "      <td>0.318655</td>\n",
       "      <td>0.254048</td>\n",
       "      <td>0.237134</td>\n",
       "      <td>0.212641</td>\n",
       "      <td>0.222496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.622757</td>\n",
       "      <td>-0.427074</td>\n",
       "      <td>-0.546902</td>\n",
       "      <td>-0.681086</td>\n",
       "      <td>-0.616897</td>\n",
       "      <td>-0.399852</td>\n",
       "      <td>-0.646469</td>\n",
       "      <td>-0.473096</td>\n",
       "      <td>-0.398657</td>\n",
       "      <td>-0.460780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026970</td>\n",
       "      <td>0.387047</td>\n",
       "      <td>0.189493</td>\n",
       "      <td>0.333211</td>\n",
       "      <td>0.316694</td>\n",
       "      <td>0.095216</td>\n",
       "      <td>0.277618</td>\n",
       "      <td>0.255762</td>\n",
       "      <td>0.634482</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.622757</td>\n",
       "      <td>-0.427074</td>\n",
       "      <td>-0.546902</td>\n",
       "      <td>-0.681086</td>\n",
       "      <td>-0.616897</td>\n",
       "      <td>-0.399852</td>\n",
       "      <td>-0.646469</td>\n",
       "      <td>-0.473096</td>\n",
       "      <td>-0.398657</td>\n",
       "      <td>-0.460780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175600</td>\n",
       "      <td>0.311160</td>\n",
       "      <td>0.309869</td>\n",
       "      <td>0.324986</td>\n",
       "      <td>0.349944</td>\n",
       "      <td>0.265524</td>\n",
       "      <td>0.263268</td>\n",
       "      <td>0.237989</td>\n",
       "      <td>0.223606</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.622757</td>\n",
       "      <td>-0.427074</td>\n",
       "      <td>-0.546902</td>\n",
       "      <td>-0.681086</td>\n",
       "      <td>-0.616897</td>\n",
       "      <td>-0.399852</td>\n",
       "      <td>-0.646469</td>\n",
       "      <td>-0.473096</td>\n",
       "      <td>-0.398657</td>\n",
       "      <td>-0.460780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148754</td>\n",
       "      <td>0.291764</td>\n",
       "      <td>0.288028</td>\n",
       "      <td>0.304497</td>\n",
       "      <td>0.324409</td>\n",
       "      <td>0.249166</td>\n",
       "      <td>0.236031</td>\n",
       "      <td>0.222158</td>\n",
       "      <td>0.211160</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.622757</td>\n",
       "      <td>-0.427074</td>\n",
       "      <td>-0.546902</td>\n",
       "      <td>-0.681086</td>\n",
       "      <td>-0.616897</td>\n",
       "      <td>-0.399852</td>\n",
       "      <td>-0.646469</td>\n",
       "      <td>-0.473096</td>\n",
       "      <td>-0.398657</td>\n",
       "      <td>-0.460780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532137</td>\n",
       "      <td>0.032426</td>\n",
       "      <td>0.061560</td>\n",
       "      <td>0.536695</td>\n",
       "      <td>0.296054</td>\n",
       "      <td>0.281888</td>\n",
       "      <td>0.210928</td>\n",
       "      <td>0.126561</td>\n",
       "      <td>0.074251</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41095</th>\n",
       "      <td>-1.167146</td>\n",
       "      <td>-0.773598</td>\n",
       "      <td>-0.408801</td>\n",
       "      <td>-0.361071</td>\n",
       "      <td>-0.592888</td>\n",
       "      <td>-0.712328</td>\n",
       "      <td>-0.773887</td>\n",
       "      <td>-0.839873</td>\n",
       "      <td>-0.909286</td>\n",
       "      <td>-0.876976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343708</td>\n",
       "      <td>0.453005</td>\n",
       "      <td>0.353923</td>\n",
       "      <td>0.161486</td>\n",
       "      <td>0.137816</td>\n",
       "      <td>0.452746</td>\n",
       "      <td>0.455927</td>\n",
       "      <td>0.430444</td>\n",
       "      <td>0.604609</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41096</th>\n",
       "      <td>-0.730467</td>\n",
       "      <td>-1.271471</td>\n",
       "      <td>-0.432713</td>\n",
       "      <td>-0.866111</td>\n",
       "      <td>-0.806027</td>\n",
       "      <td>-0.912486</td>\n",
       "      <td>-0.260361</td>\n",
       "      <td>-0.866017</td>\n",
       "      <td>-1.079413</td>\n",
       "      <td>-0.887795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280170</td>\n",
       "      <td>0.208248</td>\n",
       "      <td>-0.020355</td>\n",
       "      <td>0.457036</td>\n",
       "      <td>-0.107935</td>\n",
       "      <td>0.394542</td>\n",
       "      <td>0.382657</td>\n",
       "      <td>0.546899</td>\n",
       "      <td>0.446768</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41097</th>\n",
       "      <td>-0.451685</td>\n",
       "      <td>-0.582497</td>\n",
       "      <td>-0.289282</td>\n",
       "      <td>-0.636545</td>\n",
       "      <td>-0.051089</td>\n",
       "      <td>-0.379605</td>\n",
       "      <td>-0.474093</td>\n",
       "      <td>-0.426053</td>\n",
       "      <td>-0.249029</td>\n",
       "      <td>-0.678915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162749</td>\n",
       "      <td>0.173339</td>\n",
       "      <td>0.109635</td>\n",
       "      <td>0.151972</td>\n",
       "      <td>0.187001</td>\n",
       "      <td>0.224019</td>\n",
       "      <td>0.256327</td>\n",
       "      <td>0.299679</td>\n",
       "      <td>0.282094</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41098</th>\n",
       "      <td>-0.377553</td>\n",
       "      <td>-0.425546</td>\n",
       "      <td>-0.151302</td>\n",
       "      <td>-0.393559</td>\n",
       "      <td>-0.184409</td>\n",
       "      <td>-0.360608</td>\n",
       "      <td>-0.299241</td>\n",
       "      <td>-0.486215</td>\n",
       "      <td>-0.316570</td>\n",
       "      <td>-0.166165</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119539</td>\n",
       "      <td>0.105926</td>\n",
       "      <td>0.169582</td>\n",
       "      <td>0.327186</td>\n",
       "      <td>0.094141</td>\n",
       "      <td>0.263206</td>\n",
       "      <td>0.283964</td>\n",
       "      <td>0.270183</td>\n",
       "      <td>0.304433</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41099</th>\n",
       "      <td>-0.732623</td>\n",
       "      <td>-0.816590</td>\n",
       "      <td>-0.166635</td>\n",
       "      <td>-0.330043</td>\n",
       "      <td>-0.774980</td>\n",
       "      <td>-0.110291</td>\n",
       "      <td>-0.224574</td>\n",
       "      <td>-0.618458</td>\n",
       "      <td>-0.683957</td>\n",
       "      <td>-0.808665</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011783</td>\n",
       "      <td>0.258485</td>\n",
       "      <td>0.261268</td>\n",
       "      <td>0.366922</td>\n",
       "      <td>0.020737</td>\n",
       "      <td>0.204615</td>\n",
       "      <td>0.126698</td>\n",
       "      <td>0.327291</td>\n",
       "      <td>0.346835</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41100 rows Ã— 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.622757 -0.427074 -0.546902 -0.681086 -0.616897 -0.399852 -0.646469   \n",
       "1     -0.622757 -0.427074 -0.546902 -0.681086 -0.616897 -0.399852 -0.646469   \n",
       "2     -0.622757 -0.427074 -0.546902 -0.681086 -0.616897 -0.399852 -0.646469   \n",
       "3     -0.622757 -0.427074 -0.546902 -0.681086 -0.616897 -0.399852 -0.646469   \n",
       "4     -0.622757 -0.427074 -0.546902 -0.681086 -0.616897 -0.399852 -0.646469   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "41095 -1.167146 -0.773598 -0.408801 -0.361071 -0.592888 -0.712328 -0.773887   \n",
       "41096 -0.730467 -1.271471 -0.432713 -0.866111 -0.806027 -0.912486 -0.260361   \n",
       "41097 -0.451685 -0.582497 -0.289282 -0.636545 -0.051089 -0.379605 -0.474093   \n",
       "41098 -0.377553 -0.425546 -0.151302 -0.393559 -0.184409 -0.360608 -0.299241   \n",
       "41099 -0.732623 -0.816590 -0.166635 -0.330043 -0.774980 -0.110291 -0.224574   \n",
       "\n",
       "            7         8         9    ...       91        92        93   \\\n",
       "0     -0.473096 -0.398657 -0.460780  ...  0.153380  0.292761  0.272396   \n",
       "1     -0.473096 -0.398657 -0.460780  ...  0.026970  0.387047  0.189493   \n",
       "2     -0.473096 -0.398657 -0.460780  ...  0.175600  0.311160  0.309869   \n",
       "3     -0.473096 -0.398657 -0.460780  ...  0.148754  0.291764  0.288028   \n",
       "4     -0.473096 -0.398657 -0.460780  ...  0.532137  0.032426  0.061560   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "41095 -0.839873 -0.909286 -0.876976  ...  0.343708  0.453005  0.353923   \n",
       "41096 -0.866017 -1.079413 -0.887795  ...  0.280170  0.208248 -0.020355   \n",
       "41097 -0.426053 -0.249029 -0.678915  ...  0.162749  0.173339  0.109635   \n",
       "41098 -0.486215 -0.316570 -0.166165  ...  0.119539  0.105926  0.169582   \n",
       "41099 -0.618458 -0.683957 -0.808665  ...  0.011783  0.258485  0.261268   \n",
       "\n",
       "            94        95        96        97        98        99   100  \n",
       "0      0.308351  0.318655  0.254048  0.237134  0.212641  0.222496  0.0  \n",
       "1      0.333211  0.316694  0.095216  0.277618  0.255762  0.634482  0.0  \n",
       "2      0.324986  0.349944  0.265524  0.263268  0.237989  0.223606  0.0  \n",
       "3      0.304497  0.324409  0.249166  0.236031  0.222158  0.211160  0.0  \n",
       "4      0.536695  0.296054  0.281888  0.210928  0.126561  0.074251  0.0  \n",
       "...         ...       ...       ...       ...       ...       ...  ...  \n",
       "41095  0.161486  0.137816  0.452746  0.455927  0.430444  0.604609  1.0  \n",
       "41096  0.457036 -0.107935  0.394542  0.382657  0.546899  0.446768  1.0  \n",
       "41097  0.151972  0.187001  0.224019  0.256327  0.299679  0.282094  1.0  \n",
       "41098  0.327186  0.094141  0.263206  0.283964  0.270183  0.304433  1.0  \n",
       "41099  0.366922  0.020737  0.204615  0.126698  0.327291  0.346835  1.0  \n",
       "\n",
       "[41100 rows x 101 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enhanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='enhanced_VAE_final_new_par_50_LFM_space_3.csv'\n",
    "file_path = os.path.join(base_dir,'data','split',ltype, file_name)\n",
    "output_path = file_path\n",
    "enhanced_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
