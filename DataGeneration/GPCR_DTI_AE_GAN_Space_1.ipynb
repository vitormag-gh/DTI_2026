{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riskf\\anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\riskf\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\Users\\riskf\\anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import keras\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models, backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN-GP Architecture\n",
    "def make_generator_model(input_dim, output_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(256, activation='relu', input_dim=input_dim))\n",
    "    model.add(layers.Dense(512, activation='relu'))\n",
    "    model.add(layers.Dense(1024, activation='relu'))\n",
    "    model.add(layers.Dense(output_dim, activation='linear'))  # Linear activation for WGAN\n",
    "    return model\n",
    "\n",
    "def make_critic_model(input_dim):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(512, activation='relu', input_dim=input_dim))\n",
    "    model.add(layers.Dropout(0.1))\n",
    "    model.add(layers.Dense(256, activation='relu'))\n",
    "    model.add(layers.Dense(1))  # No activation, linear output\n",
    "    return model\n",
    "\n",
    "generator = make_generator_model(100, 101)\n",
    "critic = make_critic_model(101)\n",
    "\n",
    "# Losses and training\n",
    "def critic_loss(real_output, fake_output):\n",
    "    return tf.reduce_mean(fake_output) - tf.reduce_mean(real_output)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)\n",
    "\n",
    "def gradient_penalty(batch_size, real_images, fake_images, critic):\n",
    "    epsilon = tf.random.normal([batch_size, 1], 0.0, 1.0)\n",
    "    interpolated = epsilon * real_images + (1 - epsilon) * fake_images\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(interpolated)\n",
    "        pred = critic(interpolated, training=True)\n",
    "    grads = tape.gradient(pred, [interpolated])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1]))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "    return gp\n",
    "\n",
    "def train_step(generator, critic, batch_size, generator_optimizer, critic_optimizer, real_features):\n",
    "    # Append a label column to real_features to match the critic's input expectations\n",
    "    labels = tf.ones((batch_size, 1))  # Assume label 1 for all positive samples\n",
    "    real_data = tf.concat([real_features, labels], axis=1)\n",
    "    \n",
    "    noise = tf.random.normal([batch_size, generator.input_shape[1]])\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as crit_tape:\n",
    "        generated_data = generator(noise, training=True)\n",
    "\n",
    "        real_output = critic(real_data, training=True)\n",
    "        fake_output = critic(generated_data, training=True)\n",
    "\n",
    "        crit_loss = critic_loss(real_output, fake_output)\n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        penalty = gradient_penalty(batch_size, real_data, generated_data, critic)\n",
    "        crit_loss += 10 * penalty  # lambda for gradient penalty\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_critic = crit_tape.gradient(crit_loss, critic.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    critic_optimizer.apply_gradients(zip(gradients_of_critic, critic.trainable_variables))\n",
    "\n",
    "    return crit_loss, gen_loss\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative paths. # Set directory paths for later use.\n",
    "# Get the directory of the script file\n",
    "base_dir = os.getcwd()\n",
    "ligants_type = ['enzyme', 'GPCR', 'ion_channel', 'nuclear_receptor']\n",
    "ltype = ligants_type[1]\n",
    "file_name = 'final_new_par_50.csv'\n",
    "file_path = os.path.join(base_dir, 'data', 'split', ltype, file_name)\n",
    "data_frame = pd.read_csv(file_path, header=None, skiprows=1)\n",
    "features = data_frame.iloc[:, :-1].values\n",
    "labels = data_frame.iloc[:, -1].values\n",
    "# Filter to get only the positive samples\n",
    "positive_features = features[labels == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(635, 100)\n",
      "(21185, 100)\n",
      "(21185,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "635"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(positive_features.shape)\n",
    "print(features.shape)\n",
    "print(labels.shape)\n",
    "len(positive_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 0, Critic Loss: 2.5564253330230713, Generator Loss: 0.03385821357369423\n",
      "Epoch 0, Batch 1, Critic Loss: 2.5642354488372803, Generator Loss: -0.05705900490283966\n",
      "Epoch 1, Batch 0, Critic Loss: 2.58237886428833, Generator Loss: -0.15056344866752625\n",
      "Epoch 1, Batch 1, Critic Loss: 2.5908639430999756, Generator Loss: -0.26303568482398987\n",
      "Epoch 2, Batch 0, Critic Loss: 2.5840158462524414, Generator Loss: -0.3745231628417969\n",
      "Epoch 2, Batch 1, Critic Loss: 2.562553882598877, Generator Loss: -0.507118284702301\n",
      "Epoch 3, Batch 0, Critic Loss: 2.63327693939209, Generator Loss: -0.6742141246795654\n",
      "Epoch 3, Batch 1, Critic Loss: 2.576385021209717, Generator Loss: -0.8392695784568787\n",
      "Epoch 4, Batch 0, Critic Loss: 2.6579508781433105, Generator Loss: -1.045945167541504\n",
      "Epoch 4, Batch 1, Critic Loss: 2.6984939575195312, Generator Loss: -1.27028226852417\n",
      "Epoch 5, Batch 0, Critic Loss: 2.856401205062866, Generator Loss: -1.5610909461975098\n",
      "Epoch 5, Batch 1, Critic Loss: 3.015629291534424, Generator Loss: -1.7863801717758179\n",
      "Epoch 6, Batch 0, Critic Loss: 3.1877946853637695, Generator Loss: -2.095726490020752\n",
      "Epoch 6, Batch 1, Critic Loss: 3.456711769104004, Generator Loss: -2.4342997074127197\n",
      "Epoch 7, Batch 0, Critic Loss: 3.7009849548339844, Generator Loss: -2.7268764972686768\n",
      "Epoch 7, Batch 1, Critic Loss: 3.981480360031128, Generator Loss: -3.0428106784820557\n",
      "Epoch 8, Batch 0, Critic Loss: 4.292697429656982, Generator Loss: -3.3671717643737793\n",
      "Epoch 8, Batch 1, Critic Loss: 4.648801803588867, Generator Loss: -3.7485404014587402\n",
      "Epoch 9, Batch 0, Critic Loss: 5.0119123458862305, Generator Loss: -4.0782318115234375\n",
      "Epoch 9, Batch 1, Critic Loss: 5.149940490722656, Generator Loss: -4.211495399475098\n",
      "Epoch 10, Batch 0, Critic Loss: 5.487188339233398, Generator Loss: -4.533893585205078\n",
      "Epoch 10, Batch 1, Critic Loss: 5.652797698974609, Generator Loss: -4.68724250793457\n",
      "Epoch 11, Batch 0, Critic Loss: 5.923131942749023, Generator Loss: -4.920845031738281\n",
      "Epoch 11, Batch 1, Critic Loss: 5.993982315063477, Generator Loss: -4.971079349517822\n",
      "Epoch 12, Batch 0, Critic Loss: 6.089419364929199, Generator Loss: -4.988534927368164\n",
      "Epoch 12, Batch 1, Critic Loss: 6.141742706298828, Generator Loss: -4.9618024826049805\n",
      "Epoch 13, Batch 0, Critic Loss: 5.882796287536621, Generator Loss: -4.746870040893555\n",
      "Epoch 13, Batch 1, Critic Loss: 5.772008419036865, Generator Loss: -4.576878547668457\n",
      "Epoch 14, Batch 0, Critic Loss: 5.444293975830078, Generator Loss: -4.309500217437744\n",
      "Epoch 14, Batch 1, Critic Loss: 5.076791763305664, Generator Loss: -3.964522361755371\n",
      "Epoch 15, Batch 0, Critic Loss: 4.591980457305908, Generator Loss: -3.597015857696533\n",
      "Epoch 15, Batch 1, Critic Loss: 4.123410701751709, Generator Loss: -3.129775285720825\n",
      "Epoch 16, Batch 0, Critic Loss: 3.5909199714660645, Generator Loss: -2.6851813793182373\n",
      "Epoch 16, Batch 1, Critic Loss: 3.114471197128296, Generator Loss: -2.1979382038116455\n",
      "Epoch 17, Batch 0, Critic Loss: 2.399533271789551, Generator Loss: -1.5238037109375\n",
      "Epoch 17, Batch 1, Critic Loss: 1.8762212991714478, Generator Loss: -1.0166239738464355\n",
      "Epoch 18, Batch 0, Critic Loss: 1.413739562034607, Generator Loss: -0.5094897747039795\n",
      "Epoch 18, Batch 1, Critic Loss: 0.8343986868858337, Generator Loss: 0.12576210498809814\n",
      "Epoch 19, Batch 0, Critic Loss: 0.13651281595230103, Generator Loss: 0.7091816663742065\n",
      "Epoch 19, Batch 1, Critic Loss: -0.3300381898880005, Generator Loss: 1.1979904174804688\n",
      "Epoch 20, Batch 0, Critic Loss: -0.8792977929115295, Generator Loss: 1.7046903371810913\n",
      "Epoch 20, Batch 1, Critic Loss: -1.3586809635162354, Generator Loss: 2.193007469177246\n",
      "Epoch 21, Batch 0, Critic Loss: -1.8975048065185547, Generator Loss: 2.6640405654907227\n",
      "Epoch 21, Batch 1, Critic Loss: -2.112999200820923, Generator Loss: 2.9177396297454834\n",
      "Epoch 22, Batch 0, Critic Loss: -2.503892660140991, Generator Loss: 3.2717690467834473\n",
      "Epoch 22, Batch 1, Critic Loss: -2.7406411170959473, Generator Loss: 3.5740089416503906\n",
      "Epoch 23, Batch 0, Critic Loss: -2.9339051246643066, Generator Loss: 3.6944327354431152\n",
      "Epoch 23, Batch 1, Critic Loss: -3.10318660736084, Generator Loss: 3.8476521968841553\n",
      "Epoch 24, Batch 0, Critic Loss: -3.1986026763916016, Generator Loss: 3.9072070121765137\n",
      "Epoch 24, Batch 1, Critic Loss: -3.1533803939819336, Generator Loss: 3.8766286373138428\n",
      "Epoch 25, Batch 0, Critic Loss: -3.2303261756896973, Generator Loss: 3.916504383087158\n",
      "Epoch 25, Batch 1, Critic Loss: -3.182894706726074, Generator Loss: 3.8922624588012695\n",
      "Epoch 26, Batch 0, Critic Loss: -3.1401596069335938, Generator Loss: 3.786851167678833\n",
      "Epoch 26, Batch 1, Critic Loss: -3.0516626834869385, Generator Loss: 3.693876266479492\n",
      "Epoch 27, Batch 0, Critic Loss: -2.9907710552215576, Generator Loss: 3.576673984527588\n",
      "Epoch 27, Batch 1, Critic Loss: -2.823352098464966, Generator Loss: 3.420842409133911\n",
      "Epoch 28, Batch 0, Critic Loss: -2.786226749420166, Generator Loss: 3.3428795337677\n",
      "Epoch 28, Batch 1, Critic Loss: -2.678279399871826, Generator Loss: 3.214867353439331\n",
      "Epoch 29, Batch 0, Critic Loss: -2.5588748455047607, Generator Loss: 3.0329203605651855\n",
      "Epoch 29, Batch 1, Critic Loss: -2.3484103679656982, Generator Loss: 2.876617193222046\n",
      "Epoch 30, Batch 0, Critic Loss: -2.1194543838500977, Generator Loss: 2.6300928592681885\n",
      "Epoch 30, Batch 1, Critic Loss: -1.9825583696365356, Generator Loss: 2.528927803039551\n",
      "Epoch 31, Batch 0, Critic Loss: -1.8206605911254883, Generator Loss: 2.375297784805298\n",
      "Epoch 31, Batch 1, Critic Loss: -1.7111706733703613, Generator Loss: 2.3325300216674805\n",
      "Epoch 32, Batch 0, Critic Loss: -1.5676097869873047, Generator Loss: 2.1799845695495605\n",
      "Epoch 32, Batch 1, Critic Loss: -1.3964881896972656, Generator Loss: 2.101353168487549\n",
      "Epoch 33, Batch 0, Critic Loss: -1.3338313102722168, Generator Loss: 2.0535271167755127\n",
      "Epoch 33, Batch 1, Critic Loss: -1.2366324663162231, Generator Loss: 2.0752878189086914\n",
      "Epoch 34, Batch 0, Critic Loss: -1.1633787155151367, Generator Loss: 2.012902021408081\n",
      "Epoch 34, Batch 1, Critic Loss: -1.095219373703003, Generator Loss: 2.040710687637329\n",
      "Epoch 35, Batch 0, Critic Loss: -1.1630762815475464, Generator Loss: 2.082651376724243\n",
      "Epoch 35, Batch 1, Critic Loss: -1.0460039377212524, Generator Loss: 2.0476372241973877\n",
      "Epoch 36, Batch 0, Critic Loss: -1.0767676830291748, Generator Loss: 2.0408072471618652\n",
      "Epoch 36, Batch 1, Critic Loss: -0.9128600358963013, Generator Loss: 1.9885257482528687\n",
      "Epoch 37, Batch 0, Critic Loss: -0.9246695041656494, Generator Loss: 1.9628595113754272\n",
      "Epoch 37, Batch 1, Critic Loss: -0.7902117967605591, Generator Loss: 1.901151180267334\n",
      "Epoch 38, Batch 0, Critic Loss: -0.7195228338241577, Generator Loss: 1.814895510673523\n",
      "Epoch 38, Batch 1, Critic Loss: -0.5617184042930603, Generator Loss: 1.78328537940979\n",
      "Epoch 39, Batch 0, Critic Loss: -0.5556018948554993, Generator Loss: 1.7307796478271484\n",
      "Epoch 39, Batch 1, Critic Loss: -0.4963268041610718, Generator Loss: 1.7585444450378418\n",
      "Epoch 40, Batch 0, Critic Loss: -0.44024306535720825, Generator Loss: 1.6497682332992554\n",
      "Epoch 40, Batch 1, Critic Loss: -0.3555735945701599, Generator Loss: 1.6102259159088135\n",
      "Epoch 41, Batch 0, Critic Loss: -0.251003623008728, Generator Loss: 1.5268906354904175\n",
      "Epoch 41, Batch 1, Critic Loss: -0.0769355297088623, Generator Loss: 1.4340882301330566\n",
      "Epoch 42, Batch 0, Critic Loss: -0.01728290319442749, Generator Loss: 1.408133864402771\n",
      "Epoch 42, Batch 1, Critic Loss: 0.14468538761138916, Generator Loss: 1.3514909744262695\n",
      "Epoch 43, Batch 0, Critic Loss: 0.17554736137390137, Generator Loss: 1.2961604595184326\n",
      "Epoch 43, Batch 1, Critic Loss: 0.3270410895347595, Generator Loss: 1.2831456661224365\n",
      "Epoch 44, Batch 0, Critic Loss: 0.35833740234375, Generator Loss: 1.2587311267852783\n",
      "Epoch 44, Batch 1, Critic Loss: 0.4320441484451294, Generator Loss: 1.2727305889129639\n",
      "Epoch 45, Batch 0, Critic Loss: 0.3909209370613098, Generator Loss: 1.251978874206543\n",
      "Epoch 45, Batch 1, Critic Loss: 0.38473671674728394, Generator Loss: 1.2818832397460938\n",
      "Epoch 46, Batch 0, Critic Loss: 0.3695439100265503, Generator Loss: 1.277869462966919\n",
      "Epoch 46, Batch 1, Critic Loss: 0.4606351852416992, Generator Loss: 1.2647593021392822\n",
      "Epoch 47, Batch 0, Critic Loss: 0.48265689611434937, Generator Loss: 1.2526494264602661\n",
      "Epoch 47, Batch 1, Critic Loss: 0.5601288676261902, Generator Loss: 1.243039846420288\n",
      "Epoch 48, Batch 0, Critic Loss: 0.5525482296943665, Generator Loss: 1.214538812637329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Batch 1, Critic Loss: 0.620330274105072, Generator Loss: 1.1947033405303955\n",
      "Epoch 49, Batch 0, Critic Loss: 0.6304507851600647, Generator Loss: 1.19431471824646\n",
      "Epoch 49, Batch 1, Critic Loss: 0.6377188563346863, Generator Loss: 1.2334716320037842\n",
      "Epoch 50, Batch 0, Critic Loss: 0.6416301131248474, Generator Loss: 1.2162528038024902\n",
      "Epoch 50, Batch 1, Critic Loss: 0.6381792426109314, Generator Loss: 1.2550854682922363\n",
      "Epoch 51, Batch 0, Critic Loss: 0.6634122133255005, Generator Loss: 1.279773235321045\n",
      "Epoch 51, Batch 1, Critic Loss: 0.5445777177810669, Generator Loss: 1.2874541282653809\n",
      "Epoch 52, Batch 0, Critic Loss: 0.5827305912971497, Generator Loss: 1.2543034553527832\n",
      "Epoch 52, Batch 1, Critic Loss: 0.5703232288360596, Generator Loss: 1.2687335014343262\n",
      "Epoch 53, Batch 0, Critic Loss: 0.5234785079956055, Generator Loss: 1.2556546926498413\n",
      "Epoch 53, Batch 1, Critic Loss: 0.5060654282569885, Generator Loss: 1.217976450920105\n",
      "Epoch 54, Batch 0, Critic Loss: 0.5997961759567261, Generator Loss: 1.1321306228637695\n",
      "Epoch 54, Batch 1, Critic Loss: 0.5988030433654785, Generator Loss: 1.100612759590149\n",
      "Epoch 55, Batch 0, Critic Loss: 0.6360427141189575, Generator Loss: 1.0697829723358154\n",
      "Epoch 55, Batch 1, Critic Loss: 0.689724326133728, Generator Loss: 0.9744219779968262\n",
      "Epoch 56, Batch 0, Critic Loss: 0.7936638593673706, Generator Loss: 0.945859432220459\n",
      "Epoch 56, Batch 1, Critic Loss: 0.8584209680557251, Generator Loss: 0.8711287975311279\n",
      "Epoch 57, Batch 0, Critic Loss: 0.9382504820823669, Generator Loss: 0.8493748903274536\n",
      "Epoch 57, Batch 1, Critic Loss: 1.0154258012771606, Generator Loss: 0.8385294675827026\n",
      "Epoch 58, Batch 0, Critic Loss: 1.0421273708343506, Generator Loss: 0.8374843597412109\n",
      "Epoch 58, Batch 1, Critic Loss: 1.089977502822876, Generator Loss: 0.8737214803695679\n",
      "Epoch 59, Batch 0, Critic Loss: 1.1211105585098267, Generator Loss: 0.8808029890060425\n",
      "Epoch 59, Batch 1, Critic Loss: 1.0005662441253662, Generator Loss: 0.9255884289741516\n",
      "Epoch 60, Batch 0, Critic Loss: 1.0709391832351685, Generator Loss: 0.9527041912078857\n",
      "Epoch 60, Batch 1, Critic Loss: 0.9570277333259583, Generator Loss: 0.982532799243927\n",
      "Epoch 61, Batch 0, Critic Loss: 0.9987736344337463, Generator Loss: 1.0084474086761475\n",
      "Epoch 61, Batch 1, Critic Loss: 0.9158912897109985, Generator Loss: 1.0629911422729492\n",
      "Epoch 62, Batch 0, Critic Loss: 0.9264401793479919, Generator Loss: 1.1201895475387573\n",
      "Epoch 62, Batch 1, Critic Loss: 0.8171534538269043, Generator Loss: 1.132082462310791\n",
      "Epoch 63, Batch 0, Critic Loss: 0.8129083514213562, Generator Loss: 1.1731290817260742\n",
      "Epoch 63, Batch 1, Critic Loss: 0.6549621820449829, Generator Loss: 1.2306468486785889\n",
      "Epoch 64, Batch 0, Critic Loss: 0.7069889903068542, Generator Loss: 1.2403509616851807\n",
      "Epoch 64, Batch 1, Critic Loss: 0.5914376974105835, Generator Loss: 1.2521166801452637\n",
      "Epoch 65, Batch 0, Critic Loss: 0.545284628868103, Generator Loss: 1.2529261112213135\n",
      "Epoch 65, Batch 1, Critic Loss: 0.5077420473098755, Generator Loss: 1.2708261013031006\n",
      "Epoch 66, Batch 0, Critic Loss: 0.525557816028595, Generator Loss: 1.2546560764312744\n",
      "Epoch 66, Batch 1, Critic Loss: 0.42316484451293945, Generator Loss: 1.2363107204437256\n",
      "Epoch 67, Batch 0, Critic Loss: 0.43135643005371094, Generator Loss: 1.212857723236084\n",
      "Epoch 67, Batch 1, Critic Loss: 0.44949427247047424, Generator Loss: 1.1943132877349854\n",
      "Epoch 68, Batch 0, Critic Loss: 0.3761681020259857, Generator Loss: 1.1896240711212158\n",
      "Epoch 68, Batch 1, Critic Loss: 0.38494187593460083, Generator Loss: 1.1477129459381104\n",
      "Epoch 69, Batch 0, Critic Loss: 0.41993486881256104, Generator Loss: 1.1262768507003784\n",
      "Epoch 69, Batch 1, Critic Loss: 0.44045329093933105, Generator Loss: 1.0972009897232056\n",
      "Epoch 70, Batch 0, Critic Loss: 0.4522894024848938, Generator Loss: 1.055836796760559\n",
      "Epoch 70, Batch 1, Critic Loss: 0.4303053319454193, Generator Loss: 1.0327926874160767\n",
      "Epoch 71, Batch 0, Critic Loss: 0.4679452180862427, Generator Loss: 0.9995371699333191\n",
      "Epoch 71, Batch 1, Critic Loss: 0.49863919615745544, Generator Loss: 0.9861461520195007\n",
      "Epoch 72, Batch 0, Critic Loss: 0.48903748393058777, Generator Loss: 0.9578509330749512\n",
      "Epoch 72, Batch 1, Critic Loss: 0.4848484992980957, Generator Loss: 0.9467897415161133\n",
      "Epoch 73, Batch 0, Critic Loss: 0.5525254011154175, Generator Loss: 0.8885655403137207\n",
      "Epoch 73, Batch 1, Critic Loss: 0.5546426773071289, Generator Loss: 0.8747676610946655\n",
      "Epoch 74, Batch 0, Critic Loss: 0.5377146005630493, Generator Loss: 0.8720788955688477\n",
      "Epoch 74, Batch 1, Critic Loss: 0.5582886338233948, Generator Loss: 0.8525955677032471\n",
      "Epoch 75, Batch 0, Critic Loss: 0.5492818355560303, Generator Loss: 0.8462156653404236\n",
      "Epoch 75, Batch 1, Critic Loss: 0.5678955912590027, Generator Loss: 0.8261817693710327\n",
      "Epoch 76, Batch 0, Critic Loss: 0.5474210381507874, Generator Loss: 0.8281955122947693\n",
      "Epoch 76, Batch 1, Critic Loss: 0.5946487188339233, Generator Loss: 0.7960401773452759\n",
      "Epoch 77, Batch 0, Critic Loss: 0.5801491737365723, Generator Loss: 0.7916796803474426\n",
      "Epoch 77, Batch 1, Critic Loss: 0.6096926331520081, Generator Loss: 0.7785288095474243\n",
      "Epoch 78, Batch 0, Critic Loss: 0.6176229119300842, Generator Loss: 0.7738807201385498\n",
      "Epoch 78, Batch 1, Critic Loss: 0.6165643334388733, Generator Loss: 0.7800145745277405\n",
      "Epoch 79, Batch 0, Critic Loss: 0.567954957485199, Generator Loss: 0.7638891339302063\n",
      "Epoch 79, Batch 1, Critic Loss: 0.611257016658783, Generator Loss: 0.7573396563529968\n",
      "Epoch 80, Batch 0, Critic Loss: 0.5897822976112366, Generator Loss: 0.7497098445892334\n",
      "Epoch 80, Batch 1, Critic Loss: 0.6124727725982666, Generator Loss: 0.7466112375259399\n",
      "Epoch 81, Batch 0, Critic Loss: 0.6102124452590942, Generator Loss: 0.7368355393409729\n",
      "Epoch 81, Batch 1, Critic Loss: 0.6075985431671143, Generator Loss: 0.736748456954956\n",
      "Epoch 82, Batch 0, Critic Loss: 0.579174280166626, Generator Loss: 0.745526909828186\n",
      "Epoch 82, Batch 1, Critic Loss: 0.6105771064758301, Generator Loss: 0.7338674664497375\n",
      "Epoch 83, Batch 0, Critic Loss: 0.5875335335731506, Generator Loss: 0.7293224930763245\n",
      "Epoch 83, Batch 1, Critic Loss: 0.6130684614181519, Generator Loss: 0.7240391373634338\n",
      "Epoch 84, Batch 0, Critic Loss: 0.5829981565475464, Generator Loss: 0.7217943668365479\n",
      "Epoch 84, Batch 1, Critic Loss: 0.6036878824234009, Generator Loss: 0.7329938411712646\n",
      "Epoch 85, Batch 0, Critic Loss: 0.5763804316520691, Generator Loss: 0.7219848036766052\n",
      "Epoch 85, Batch 1, Critic Loss: 0.5986757874488831, Generator Loss: 0.7194631695747375\n",
      "Epoch 86, Batch 0, Critic Loss: 0.5795213580131531, Generator Loss: 0.7059680223464966\n",
      "Epoch 86, Batch 1, Critic Loss: 0.6435544490814209, Generator Loss: 0.6963473558425903\n",
      "Epoch 87, Batch 0, Critic Loss: 0.6007969975471497, Generator Loss: 0.6903768181800842\n",
      "Epoch 87, Batch 1, Critic Loss: 0.5957868695259094, Generator Loss: 0.6940703988075256\n",
      "Epoch 88, Batch 0, Critic Loss: 0.5759527683258057, Generator Loss: 0.6898279190063477\n",
      "Epoch 88, Batch 1, Critic Loss: 0.6276161074638367, Generator Loss: 0.6810094714164734\n",
      "Epoch 89, Batch 0, Critic Loss: 0.606380820274353, Generator Loss: 0.6775052547454834\n",
      "Epoch 89, Batch 1, Critic Loss: 0.6404569149017334, Generator Loss: 0.6717120409011841\n",
      "Epoch 90, Batch 0, Critic Loss: 0.5812498331069946, Generator Loss: 0.6823988556861877\n",
      "Epoch 90, Batch 1, Critic Loss: 0.6127336621284485, Generator Loss: 0.6746215224266052\n",
      "Epoch 91, Batch 0, Critic Loss: 0.605571985244751, Generator Loss: 0.6611860990524292\n",
      "Epoch 91, Batch 1, Critic Loss: 0.6135669350624084, Generator Loss: 0.6598048210144043\n",
      "Epoch 92, Batch 0, Critic Loss: 0.5866631865501404, Generator Loss: 0.6606525778770447\n",
      "Epoch 92, Batch 1, Critic Loss: 0.6019129753112793, Generator Loss: 0.6608141660690308\n",
      "Epoch 93, Batch 0, Critic Loss: 0.6016242504119873, Generator Loss: 0.6483510732650757\n",
      "Epoch 93, Batch 1, Critic Loss: 0.6226805448532104, Generator Loss: 0.6524688005447388\n",
      "Epoch 94, Batch 0, Critic Loss: 0.6056155562400818, Generator Loss: 0.6330549716949463\n",
      "Epoch 94, Batch 1, Critic Loss: 0.611207127571106, Generator Loss: 0.6409223675727844\n",
      "Epoch 95, Batch 0, Critic Loss: 0.572006106376648, Generator Loss: 0.6380904912948608\n",
      "Epoch 95, Batch 1, Critic Loss: 0.6117769479751587, Generator Loss: 0.6288344264030457\n",
      "Epoch 96, Batch 0, Critic Loss: 0.6024710536003113, Generator Loss: 0.623536229133606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, Batch 1, Critic Loss: 0.6491603851318359, Generator Loss: 0.6258448362350464\n",
      "Epoch 97, Batch 0, Critic Loss: 0.5969604253768921, Generator Loss: 0.6202605962753296\n",
      "Epoch 97, Batch 1, Critic Loss: 0.6386134624481201, Generator Loss: 0.6194424629211426\n",
      "Epoch 98, Batch 0, Critic Loss: 0.5793331861495972, Generator Loss: 0.6172288060188293\n",
      "Epoch 98, Batch 1, Critic Loss: 0.6307359933853149, Generator Loss: 0.6057853102684021\n",
      "Epoch 99, Batch 0, Critic Loss: 0.5985867977142334, Generator Loss: 0.6053241491317749\n",
      "Epoch 99, Batch 1, Critic Loss: 0.6174975633621216, Generator Loss: 0.6088495254516602\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_samples_to_generate = 19915\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "learning_rate=0.0001\n",
    "beta_1=0.5\n",
    "generator_optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "critic_optimizer = Adam(learning_rate=learning_rate, beta_1=beta_1)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(0, len(positive_features), batch_size):\n",
    "        real_data_batch = positive_features[batch:batch + batch_size]\n",
    "        if real_data_batch.shape[0] != batch_size:  # Handle last batch which may be smaller\n",
    "            continue  # Skip if the batch isn't full size\n",
    "        crit_loss, gen_loss = train_step(generator, critic, batch_size, generator_optimizer, critic_optimizer, real_data_batch)\n",
    "        print(f'Epoch {epoch}, Batch {batch // batch_size}, Critic Loss: {crit_loss.numpy()}, Generator Loss: {gen_loss.numpy()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the last column name in  original dataframe represents the label\n",
    "all_column_names = data_frame.columns.tolist()  # This should have 101 names if the label is included in data_frame\n",
    "\n",
    "# Generate synthetic data\n",
    "noise = tf.random.normal([num_samples_to_generate, 100])\n",
    "synthetic_data = generator(noise, training=False)\n",
    "synthetic_data_df = pd.DataFrame(synthetic_data.numpy(), columns=all_column_names)\n",
    "# Set the label for all generated data to 1\n",
    "synthetic_data_df[all_column_names[-1]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#synthetic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine original and synthetic data\n",
    "enhanced_df = pd.concat([data_frame, synthetic_data_df], axis=0).reset_index(drop=True)\n",
    "file_name='enhanced_GAN_final_new_par_50_space_1.csv'\n",
    "file_path = os.path.join(base_dir,'data','split',ltype, file_name)\n",
    "output_path = file_path\n",
    "enhanced_df.to_csv(output_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
